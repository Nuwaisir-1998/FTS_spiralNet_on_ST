{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.14"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.14 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import functools\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "# from .src.model.loss import VGG19\n",
    "\n",
    "\n",
    "class VGG19(torch.nn.Module):\n",
    "    def __init__(self, stage):\n",
    "        super(VGG19, self).__init__()\n",
    "        vgg19 = models.vgg19(pretrained=True)\n",
    "        features = vgg19.features\n",
    "        self.stage = stage\n",
    "\n",
    "        self.relu1_1 = torch.nn.Sequential()\n",
    "        self.relu1_2 = torch.nn.Sequential()\n",
    "\n",
    "        self.relu2_1 = torch.nn.Sequential()\n",
    "        self.relu2_2 = torch.nn.Sequential()\n",
    "\n",
    "        self.relu3_1 = torch.nn.Sequential()\n",
    "        self.relu3_2 = torch.nn.Sequential()\n",
    "        self.relu3_3 = torch.nn.Sequential()\n",
    "        self.relu3_4 = torch.nn.Sequential()\n",
    "\n",
    "        self.relu4_1 = torch.nn.Sequential()\n",
    "        self.relu4_2 = torch.nn.Sequential()\n",
    "        self.relu4_3 = torch.nn.Sequential()\n",
    "        self.relu4_4 = torch.nn.Sequential()\n",
    "\n",
    "        self.relu5_1 = torch.nn.Sequential()\n",
    "        self.relu5_2 = torch.nn.Sequential()\n",
    "        self.relu5_3 = torch.nn.Sequential()\n",
    "        self.relu5_4 = torch.nn.Sequential()\n",
    "        \n",
    "        if self.stage == 1:\n",
    "            classifier = vgg19.classifier\n",
    "            self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "            self.class_other = torch.nn.Sequential()\n",
    "            self.linear_3 = torch.nn.Sequential()\n",
    "\n",
    "        for x in range(2):\n",
    "            self.relu1_1.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(2, 4):\n",
    "            self.relu1_2.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(4, 7):\n",
    "            self.relu2_1.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(7, 9):\n",
    "            self.relu2_2.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(9, 12):\n",
    "            self.relu3_1.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(12, 14):\n",
    "            self.relu3_2.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(14, 16):\n",
    "            self.relu3_2.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(16, 18):\n",
    "            self.relu3_4.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(18, 21):\n",
    "            self.relu4_1.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(21, 23):\n",
    "            self.relu4_2.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(23, 25):\n",
    "            self.relu4_3.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(25, 27):\n",
    "            self.relu4_4.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(27, 30):\n",
    "            self.relu5_1.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(30, 32):\n",
    "            self.relu5_2.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(32, 34):\n",
    "            self.relu5_3.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(34, 36):\n",
    "            self.relu5_4.add_module(str(x), features[x])\n",
    "        \n",
    "        if self.stage == 1:\n",
    "            for x in range(0, 6):\n",
    "                self.class_other.add_module(str(x), classifier[x])\n",
    "            for x in range(6, 6):\n",
    "                self.linear_3.add_module(str(x), classifier[x])\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        relu1_1 = self.relu1_1(x)\n",
    "        relu1_2 = self.relu1_2(relu1_1)\n",
    "\n",
    "        relu2_1 = self.relu2_1(relu1_2)\n",
    "        relu2_2 = self.relu2_2(relu2_1)\n",
    "\n",
    "        relu3_1 = self.relu3_1(relu2_2)\n",
    "        relu3_2 = self.relu3_2(relu3_1)\n",
    "        relu3_3 = self.relu3_3(relu3_2)\n",
    "        relu3_4 = self.relu3_4(relu3_3)\n",
    "\n",
    "        relu4_1 = self.relu4_1(relu3_4)\n",
    "        relu4_2 = self.relu4_2(relu4_1)\n",
    "        relu4_3 = self.relu4_3(relu4_2)\n",
    "        relu4_4 = self.relu4_4(relu4_3)\n",
    "\n",
    "        relu5_1 = self.relu5_1(relu4_4)\n",
    "        relu5_2 = self.relu5_2(relu5_1)\n",
    "        relu5_3 = self.relu5_3(relu5_2)\n",
    "        relu5_4 = self.relu5_4(relu5_3)\n",
    "        \n",
    "        linear_3 = None\n",
    "        if self.stage == 1:\n",
    "            maxpool = self.maxpool(relu5_4)\n",
    "            x = maxpool.view(maxpool.shape[0],-1)\n",
    "            x = self.class_other(x)\n",
    "            linear_3 = self.linear_3(x)\n",
    "        \n",
    "        out = {\n",
    "            'relu1_1': relu1_1,\n",
    "            'relu1_2': relu1_2,\n",
    "\n",
    "            'relu2_1': relu2_1,\n",
    "            'relu2_2': relu2_2,\n",
    "\n",
    "            'relu3_1': relu3_1,\n",
    "            'relu3_2': relu3_2,\n",
    "            'relu3_3': relu3_3,\n",
    "            'relu3_4': relu3_4,\n",
    "\n",
    "            'relu4_1': relu4_1,\n",
    "            'relu4_2': relu4_2,\n",
    "            'relu4_3': relu4_3,\n",
    "            'relu4_4': relu4_4,\n",
    "\n",
    "            'relu5_1': relu5_1,\n",
    "            'relu5_2': relu5_2,\n",
    "            'relu5_3': relu5_3,\n",
    "            'relu5_4': relu5_4,\n",
    "\n",
    "            'linear_3': linear_3\n",
    "        }\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class BaseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseNetwork, self).__init__()\n",
    "\n",
    "    def init_weights(self, init_type='xavier', gain=0.02):\n",
    "        def init_func(m):\n",
    "            classname = m.__class__.__name__\n",
    "            if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "                if init_type == 'normal':\n",
    "                    nn.init.normal_(m.weight.data, 0.0, gain)\n",
    "                elif init_type == 'xavier':\n",
    "                    nn.init.xavier_normal_(m.weight.data, gain=gain)\n",
    "                elif init_type == 'kaiming':\n",
    "                    nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "                elif init_type == 'orthogonal':\n",
    "                    nn.init.orthogonal_(m.weight.data, gain=gain)\n",
    "\n",
    "                if hasattr(m, 'bias') and m.bias is not None:\n",
    "                    nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "            elif classname.find('BatchNorm2d') != -1:\n",
    "                nn.init.normal_(m.weight.data, 1.0, gain)\n",
    "                nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "        self.apply(init_func)\n",
    "        \n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim, dilation=1, use_spectral_norm=False, use_dropout=False):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        conv_block = [\n",
    "            nn.ReflectionPad2d(dilation),\n",
    "            spectral_norm(nn.Conv2d(in_channels=dim, out_channels=dim, kernel_size=3, padding=0, dilation=dilation, bias=not use_spectral_norm), use_spectral_norm),\n",
    "            nn.InstanceNorm2d(dim, track_running_stats=False),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ReflectionPad2d(1),\n",
    "            spectral_norm(nn.Conv2d(in_channels=dim, out_channels=dim, kernel_size=3, padding=0, dilation=1, bias=not use_spectral_norm), use_spectral_norm),\n",
    "            nn.InstanceNorm2d(dim, track_running_stats=False),\n",
    "         ]\n",
    "        \n",
    "        if use_dropout:\n",
    "            conv_block += [nn.Dropout(0.5)]\n",
    "            \n",
    "        self.conv_block = nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.conv_block(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "def spectral_norm(module, mode=True):\n",
    "    if mode:\n",
    "        return nn.utils.spectral_norm(module)\n",
    "\n",
    "    return module\n",
    "    \n",
    "class ImagineNet(BaseNetwork):\n",
    "    def __init__(self, residual_blocks=8, init_weights=True, in_channels=3, out_channels=3, expand=True):\n",
    "        super(ImagineNet, self).__init__()\n",
    "        \n",
    "        # self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=64, kernel_size=7, stride=1, padding=0),\n",
    "            nn.InstanceNorm2d(64, track_running_stats=False),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128, track_running_stats=False),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256, track_running_stats=False),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        blocks = []\n",
    "        for _ in range(residual_blocks):\n",
    "            block = ResnetBlock(256, 2)\n",
    "            blocks.append(block)\n",
    "\n",
    "        self.middle = nn.Sequential(*blocks)\n",
    "        \n",
    "        self.outer = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(128, track_running_stats=False),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(64, track_running_stats=False),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(in_channels=64, out_channels=3, kernel_size=7, padding=0),\n",
    "        )\n",
    "        self.tanh = nn.Tanh()\n",
    "            \n",
    "        if init_weights:\n",
    "            self.init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.middle(x)\n",
    "        x = self.outer(x)\n",
    "        x = (self.tanh(x) + 1) / 2\n",
    "        return x\n",
    "\n",
    "\n",
    "class Dis_Imagine(BaseNetwork):\n",
    "    def __init__(self, in_channels, use_sigmoid=True, use_spectral_norm=True, init_weights=True):\n",
    "        super(Dis_Imagine, self).__init__()\n",
    "        self.use_sigmoid = use_sigmoid\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(in_channels=in_channels, out_channels=64, kernel_size=4, stride=2, padding=1, bias=not use_spectral_norm), use_spectral_norm),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1, bias=not use_spectral_norm), use_spectral_norm),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1, bias=not use_spectral_norm), use_spectral_norm),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=1, padding=1, bias=not use_spectral_norm), use_spectral_norm),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.conv5 = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(in_channels=512, out_channels=1, kernel_size=4, stride=1, padding=1, bias=not use_spectral_norm), use_spectral_norm),\n",
    "        )\n",
    "\n",
    "        if init_weights:\n",
    "            self.init_weights()\n",
    "     \n",
    "    def forward(self, x):\n",
    "        conv1 = self.conv1(x)\n",
    "        conv2 = self.conv2(conv1)\n",
    "        conv3 = self.conv3(conv2)\n",
    "        conv4 = self.conv4(conv3)\n",
    "        conv5 = self.conv5(conv4)\n",
    "\n",
    "        outputs = conv5\n",
    "        if self.use_sigmoid:\n",
    "            outputs = torch.sigmoid(conv5)\n",
    "\n",
    "        return outputs, [conv1, conv2, conv3, conv4, conv5]\n",
    "class SpiralNet(BaseNetwork):\n",
    "    def __init__(self, out_size, device, ks=3, each=16):\n",
    "        super(SpiralNet, self).__init__()\n",
    "\n",
    "        self.ks, self.out_size, self.device, self.each = ks, out_size, device, each\n",
    "\n",
    "        self.G = Gen(inc=3, outc=3, mid=64)\n",
    "\n",
    "        self.init_weights()\n",
    "        self.strid = self.each  # must divide by 4\n",
    "\n",
    "        self.wmax, self.hmax = self.out_size[0], self.out_size[1]\n",
    "\n",
    "    def cat_sf(self, x, sfdata, p, direct):\n",
    "        h, w = x.shape[2], x.shape[3]\n",
    "\n",
    "        if direct == 0:\n",
    "            crop_sf = sfdata[:, :, p[1]:p[1] + h, p[0]:p[0] + w]\n",
    "            slice_in = torch.cat((crop_sf, x), dim=3)\n",
    "            return crop_sf\n",
    "        elif direct == 1:\n",
    "            crop_sf = sfdata[:, :, p[1]:p[1] + h, p[0]:p[0] + w]\n",
    "            slice_in = torch.cat((crop_sf, x), dim=2)\n",
    "            return crop_sf\n",
    "        elif direct == 2:\n",
    "            crop_sf = sfdata[:, :, p[1] - h:p[1], p[0] - w:p[0]]\n",
    "            slice_in = torch.cat((x, crop_sf), dim=3)\n",
    "            return crop_sf\n",
    "        elif direct == 3:\n",
    "            crop_sf = sfdata[:, :, p[1] - h:p[1], p[0] - w:p[0]]\n",
    "            slice_in = torch.cat((x, crop_sf), dim=2)\n",
    "            return crop_sf\n",
    "\n",
    "    def sliceOperator_1(self, x, direct):\n",
    "        # direct => l/u/r/d: 0,1,2,3\n",
    "        h, w = x.shape[2], x.shape[3]\n",
    "        if direct == 0:\n",
    "            return x[:, :, :, :self.each]\n",
    "        if direct == 1:\n",
    "            return x[:, :, :self.each, :]\n",
    "        if direct == 2:\n",
    "            return x[:, :, :, -self.each:]\n",
    "        if direct == 3:\n",
    "            return x[:, :, -self.each:, :]\n",
    "\n",
    "    def extrapolateOperator(self, x, direct, conv_data, loc):\n",
    "        cat_edge = self.each\n",
    "        if direct == 0:\n",
    "            if loc < self.each:\n",
    "                cat_edge = loc - 0\n",
    "            x = torch.cat((conv_data[:, :, :, :cat_edge], x), dim=3)\n",
    "        if direct == 1:\n",
    "            if loc < self.each:\n",
    "                cat_edge = loc - 0\n",
    "            x = torch.cat((conv_data[:, :, :cat_edge, :], x), dim=2)\n",
    "        if direct == 2:\n",
    "            if loc > (self.wmax - self.each):\n",
    "                cat_edge = self.wmax - loc\n",
    "            x = torch.cat((x, conv_data[:, :, :, -cat_edge:]), dim=3)\n",
    "        if direct == 3:\n",
    "            if loc > (self.hmax - self.each):\n",
    "                cat_edge = self.hmax - loc\n",
    "            x = torch.cat((x, conv_data[:, :, -cat_edge:, :]), dim=2)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def sliceOperator_from(self, x, direct, gt, coord):\n",
    "        h, w = x.shape[2], x.shape[3]\n",
    "        x1, y1, x2, y2 = coord[0], coord[1], coord[2], coord[3]\n",
    "        if direct == 0:\n",
    "            return gt[:, :, y1:y1 + h, x1:x1 + self.each]\n",
    "        if direct == 1:\n",
    "            return gt[:, :, y1:y1 + self.each, x1:x1 + w]\n",
    "        if direct == 2:\n",
    "            return gt[:, :, y2 - h:y2, x2 - self.each:x2]\n",
    "        if direct == 3:\n",
    "            return gt[:, :, y2 - self.each:y2, x1:x1 + w]\n",
    "\n",
    "    def sliceGenerator(self, stg, coord, inits, subimage, fm, each, direct=0, post_train=False):\n",
    "        iner = subimage\n",
    "        if direct == 0:\n",
    "            if coord[0] > 0:\n",
    "                loc_x1 = coord[0]\n",
    "                coord[0] -= each\n",
    "                coord[0] = coord[0] if coord[0] > 0 else 0\n",
    "                dir = [1, 0, 0, 0]\n",
    "                stg = stg + dir\n",
    "                c_left = self.sliceOperator_1(iner, direct)\n",
    "                if not post_train:\n",
    "                    styslice = self.sliceOperator_from(c_left, direct, gt, coord)\n",
    "                else:\n",
    "                    styslice = self.sliceOperator_1(c_left, direct)\n",
    "                slice_in = self.cat_sf(c_left, fm, (coord[0], coord[1]), direct)\n",
    "                l = self.G(slice_in, styslice, inits, stg)\n",
    "                iner = self.extrapolateOperator(iner, direct, l, loc_x1)\n",
    "                return iner, coord[0], coord[1]\n",
    "\n",
    "            else:\n",
    "                return iner, coord[0], coord[1]\n",
    "\n",
    "        elif direct == 1:\n",
    "            if coord[1] > 0:\n",
    "                loc_y1 = coord[1]\n",
    "                coord[1] -= each\n",
    "                coord[1] = coord[1] if coord[1] > 0 else 0\n",
    "                dir = [0, 1, 0, 0]\n",
    "                stg = stg + dir\n",
    "                c_up = self.sliceOperator_1(iner, direct)\n",
    "                if not post_train:\n",
    "                    styslice = self.sliceOperator_from(c_up, direct, gt, coord)\n",
    "                else:\n",
    "                    styslice = self.sliceOperator_1(c_up, direct)\n",
    "                slice_in = self.cat_sf(c_up, fm, (coord[0], coord[1]), direct)\n",
    "                u = self.G(slice_in, styslice, inits, stg)\n",
    "                iner = self.extrapolateOperator(iner, direct, u, loc_y1)\n",
    "                return iner, coord[0], coord[1]\n",
    "            else:\n",
    "                return iner, coord[0], coord[1]\n",
    "\n",
    "        elif direct == 2:\n",
    "            if coord[2] < self.wmax:\n",
    "                loc_x2 = coord[2]\n",
    "                coord[2] += each\n",
    "                coord[2] = coord[2] if coord[2] < self.wmax else self.wmax\n",
    "                dir = [0, 0, 1, 0]\n",
    "                stg = stg + dir\n",
    "                c_right = self.sliceOperator_1(iner, direct)\n",
    "                if not post_train:\n",
    "                    styslice = self.sliceOperator_from(c_right, direct, gt, coord)\n",
    "                else:\n",
    "                    styslice = self.sliceOperator_1(c_right, direct)\n",
    "                slice_in = self.cat_sf(c_right, fm, (coord[2], coord[3]), direct)\n",
    "                r = self.G(slice_in, styslice, inits, stg)\n",
    "                iner = self.extrapolateOperator(iner, direct, r, loc_x2)\n",
    "                return iner, coord[2], coord[3]\n",
    "            else:\n",
    "                return iner, coord[2], coord[3]\n",
    "\n",
    "        elif direct == 3:\n",
    "            if coord[3] < self.hmax:\n",
    "                loc_y2 = coord[3]\n",
    "                coord[3] += each\n",
    "                coord[3] = coord[3] if coord[3] < self.hmax else self.hmax\n",
    "                dir = [0, 0, 0, 1]\n",
    "                stg = stg + dir\n",
    "                c_down = self.sliceOperator_1(iner, direct)\n",
    "                if not post_train:\n",
    "                    styslice = self.sliceOperator_from(c_down, direct, gt, coord)\n",
    "                else:\n",
    "                    styslice = self.sliceOperator_1(c_down, direct)\n",
    "                slice_in = self.cat_sf(c_down, fm, (coord[2], coord[3]), direct)\n",
    "                d = self.G(slice_in, styslice, inits, stg)\n",
    "                iner = self.extrapolateOperator(iner, direct, d, loc_y2)\n",
    "                return iner, coord[2], coord[3]\n",
    "            else:\n",
    "                return iner, coord[2], coord[3]\n",
    "\n",
    "    def forward(self, x, gt, fm, position, stage, post_train=False):\n",
    "        x1, y1, x2, y2 = position[0][0].item(), position[0][1].item(), position[1][0].item(), position[1][1].item()\n",
    "        inits = x\n",
    "        coord_all = []\n",
    "        post_train = post_train\n",
    "\n",
    "        for st in range(int(stage)):\n",
    "            gen = x\n",
    "            stg = [0, 0, 0, 0]\n",
    "            gen, x1, y1 = self.sliceGenerator(stg, [x1, y1, x2, y2], inits, gen, fm, self.each, direct=0,\n",
    "                                        post_train=post_train)\n",
    "            coord_all.append([x1, y1, x2, y2])\n",
    "            gen, x1, y1 = self.sliceGenerator(stg, [x1, y1, x2, y2], inits, gen, fm, self.each, direct=1,\n",
    "                                        post_train=post_train)\n",
    "            coord_all.append([x1, y1, x2, y2])\n",
    "            gen, x2, y2 = self.sliceGenerator(stg, [x1, y1, x2, y2], inits, gen, fm, self.each, direct=2,\n",
    "                                        post_train=post_train)\n",
    "            coord_all.append([x1, y1, x2, y2])\n",
    "            gen, x2, y2 = self.sliceGenerator(stg, [x1, y1, x2, y2], inits, gen, fm, self.each, direct=3,\n",
    "                                        post_train=post_train)\n",
    "            coord_all.append([x1, y1, x2, y2])\n",
    "\n",
    "            x = gen\n",
    "\n",
    "        return x, coord_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dis_Imagine(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (conv4): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (conv5): Sequential(\n",
       "    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "# ImagineNet()\n",
    "Dis_Imagine(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n   ReflectionPad2d-1            [-1, 3, 72, 72]               0\n            Conv2d-2           [-1, 64, 66, 66]           9,472\n    InstanceNorm2d-3           [-1, 64, 66, 66]               0\n              ReLU-4           [-1, 64, 66, 66]               0\n            Conv2d-5          [-1, 128, 33, 33]          73,856\n    InstanceNorm2d-6          [-1, 128, 33, 33]               0\n              ReLU-7          [-1, 128, 33, 33]               0\n            Conv2d-8          [-1, 256, 17, 17]         295,168\n    InstanceNorm2d-9          [-1, 256, 17, 17]               0\n             ReLU-10          [-1, 256, 17, 17]               0\n  ReflectionPad2d-11          [-1, 256, 21, 21]               0\n           Conv2d-12          [-1, 256, 17, 17]         590,080\n   InstanceNorm2d-13          [-1, 256, 17, 17]               0\n             ReLU-14          [-1, 256, 17, 17]               0\n  ReflectionPad2d-15          [-1, 256, 19, 19]               0\n           Conv2d-16          [-1, 256, 17, 17]         590,080\n   InstanceNorm2d-17          [-1, 256, 17, 17]               0\n      ResnetBlock-18          [-1, 256, 17, 17]               0\n  ReflectionPad2d-19          [-1, 256, 21, 21]               0\n           Conv2d-20          [-1, 256, 17, 17]         590,080\n   InstanceNorm2d-21          [-1, 256, 17, 17]               0\n             ReLU-22          [-1, 256, 17, 17]               0\n  ReflectionPad2d-23          [-1, 256, 19, 19]               0\n           Conv2d-24          [-1, 256, 17, 17]         590,080\n   InstanceNorm2d-25          [-1, 256, 17, 17]               0\n      ResnetBlock-26          [-1, 256, 17, 17]               0\n  ReflectionPad2d-27          [-1, 256, 21, 21]               0\n           Conv2d-28          [-1, 256, 17, 17]         590,080\n   InstanceNorm2d-29          [-1, 256, 17, 17]               0\n             ReLU-30          [-1, 256, 17, 17]               0\n  ReflectionPad2d-31          [-1, 256, 19, 19]               0\n           Conv2d-32          [-1, 256, 17, 17]         590,080\n   InstanceNorm2d-33          [-1, 256, 17, 17]               0\n      ResnetBlock-34          [-1, 256, 17, 17]               0\n  ReflectionPad2d-35          [-1, 256, 21, 21]               0\n           Conv2d-36          [-1, 256, 17, 17]         590,080\n   InstanceNorm2d-37          [-1, 256, 17, 17]               0\n             ReLU-38          [-1, 256, 17, 17]               0\n  ReflectionPad2d-39          [-1, 256, 19, 19]               0\n           Conv2d-40          [-1, 256, 17, 17]         590,080\n   InstanceNorm2d-41          [-1, 256, 17, 17]               0\n      ResnetBlock-42          [-1, 256, 17, 17]               0\n  ReflectionPad2d-43          [-1, 256, 21, 21]               0\n           Conv2d-44          [-1, 256, 17, 17]         590,080\n   InstanceNorm2d-45          [-1, 256, 17, 17]               0\n             ReLU-46          [-1, 256, 17, 17]               0\n  ReflectionPad2d-47          [-1, 256, 19, 19]               0\n           Conv2d-48          [-1, 256, 17, 17]         590,080\n   InstanceNorm2d-49          [-1, 256, 17, 17]               0\n      ResnetBlock-50          [-1, 256, 17, 17]               0\n  ReflectionPad2d-51          [-1, 256, 21, 21]               0\n           Conv2d-52          [-1, 256, 17, 17]         590,080\n   InstanceNorm2d-53          [-1, 256, 17, 17]               0\n             ReLU-54          [-1, 256, 17, 17]               0\n  ReflectionPad2d-55          [-1, 256, 19, 19]               0\n           Conv2d-56          [-1, 256, 17, 17]         590,080\n   InstanceNorm2d-57          [-1, 256, 17, 17]               0\n      ResnetBlock-58          [-1, 256, 17, 17]               0\n  ReflectionPad2d-59          [-1, 256, 21, 21]               0\n           Conv2d-60          [-1, 256, 17, 17]         590,080\n   InstanceNorm2d-61          [-1, 256, 17, 17]               0\n             ReLU-62          [-1, 256, 17, 17]               0\n  ReflectionPad2d-63          [-1, 256, 19, 19]               0\n           Conv2d-64          [-1, 256, 17, 17]         590,080\n   InstanceNorm2d-65          [-1, 256, 17, 17]               0\n      ResnetBlock-66          [-1, 256, 17, 17]               0\n  ReflectionPad2d-67          [-1, 256, 21, 21]               0\n           Conv2d-68          [-1, 256, 17, 17]         590,080\n   InstanceNorm2d-69          [-1, 256, 17, 17]               0\n             ReLU-70          [-1, 256, 17, 17]               0\n  ReflectionPad2d-71          [-1, 256, 19, 19]               0\n           Conv2d-72          [-1, 256, 17, 17]         590,080\n   InstanceNorm2d-73          [-1, 256, 17, 17]               0\n      ResnetBlock-74          [-1, 256, 17, 17]               0\n  ConvTranspose2d-75          [-1, 128, 34, 34]         295,040\n   InstanceNorm2d-76          [-1, 128, 34, 34]               0\n             ReLU-77          [-1, 128, 34, 34]               0\n  ConvTranspose2d-78           [-1, 64, 68, 68]          73,792\n   InstanceNorm2d-79           [-1, 64, 68, 68]               0\n             ReLU-80           [-1, 64, 68, 68]               0\n  ReflectionPad2d-81           [-1, 64, 74, 74]               0\n           Conv2d-82            [-1, 3, 68, 68]           9,411\n             Tanh-83            [-1, 3, 68, 68]               0\n================================================================\nTotal params: 10,198,019\nTrainable params: 10,198,019\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.05\nForward/backward pass size (MB): 64.05\nParams size (MB): 38.90\nEstimated Total Size (MB): 103.01\n----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "net = ImagineNet()\n",
    "# net = Dis_Imagine(6)\n",
    "summary(net, (3, 66, 66))\n",
    "# summary(net, (6, 66, 66))"
   ]
  }
 ]
}