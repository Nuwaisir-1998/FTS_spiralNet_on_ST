{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.14"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.14 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import functools\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "# from .src.model.loss import VGG19\n",
    "\n",
    "\n",
    "class VGG19(torch.nn.Module):\n",
    "    def __init__(self, stage):\n",
    "        super(VGG19, self).__init__()\n",
    "        vgg19 = models.vgg19(pretrained=True)\n",
    "        features = vgg19.features\n",
    "        self.stage = stage\n",
    "\n",
    "        self.relu1_1 = torch.nn.Sequential()\n",
    "        self.relu1_2 = torch.nn.Sequential()\n",
    "\n",
    "        self.relu2_1 = torch.nn.Sequential()\n",
    "        self.relu2_2 = torch.nn.Sequential()\n",
    "\n",
    "        self.relu3_1 = torch.nn.Sequential()\n",
    "        self.relu3_2 = torch.nn.Sequential()\n",
    "        self.relu3_3 = torch.nn.Sequential()\n",
    "        self.relu3_4 = torch.nn.Sequential()\n",
    "\n",
    "        self.relu4_1 = torch.nn.Sequential()\n",
    "        self.relu4_2 = torch.nn.Sequential()\n",
    "        self.relu4_3 = torch.nn.Sequential()\n",
    "        self.relu4_4 = torch.nn.Sequential()\n",
    "\n",
    "        self.relu5_1 = torch.nn.Sequential()\n",
    "        self.relu5_2 = torch.nn.Sequential()\n",
    "        self.relu5_3 = torch.nn.Sequential()\n",
    "        self.relu5_4 = torch.nn.Sequential()\n",
    "        \n",
    "        if self.stage == 1:\n",
    "            classifier = vgg19.classifier\n",
    "            self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "            self.class_other = torch.nn.Sequential()\n",
    "            self.linear_3 = torch.nn.Sequential()\n",
    "\n",
    "        for x in range(2):\n",
    "            self.relu1_1.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(2, 4):\n",
    "            self.relu1_2.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(4, 7):\n",
    "            self.relu2_1.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(7, 9):\n",
    "            self.relu2_2.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(9, 12):\n",
    "            self.relu3_1.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(12, 14):\n",
    "            self.relu3_2.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(14, 16):\n",
    "            self.relu3_2.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(16, 18):\n",
    "            self.relu3_4.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(18, 21):\n",
    "            self.relu4_1.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(21, 23):\n",
    "            self.relu4_2.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(23, 25):\n",
    "            self.relu4_3.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(25, 27):\n",
    "            self.relu4_4.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(27, 30):\n",
    "            self.relu5_1.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(30, 32):\n",
    "            self.relu5_2.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(32, 34):\n",
    "            self.relu5_3.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(34, 36):\n",
    "            self.relu5_4.add_module(str(x), features[x])\n",
    "        \n",
    "        if self.stage == 1:\n",
    "            for x in range(0, 6):\n",
    "                self.class_other.add_module(str(x), classifier[x])\n",
    "            for x in range(6, 6):\n",
    "                self.linear_3.add_module(str(x), classifier[x])\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        relu1_1 = self.relu1_1(x)\n",
    "        relu1_2 = self.relu1_2(relu1_1)\n",
    "\n",
    "        relu2_1 = self.relu2_1(relu1_2)\n",
    "        relu2_2 = self.relu2_2(relu2_1)\n",
    "\n",
    "        relu3_1 = self.relu3_1(relu2_2)\n",
    "        relu3_2 = self.relu3_2(relu3_1)\n",
    "        relu3_3 = self.relu3_3(relu3_2)\n",
    "        relu3_4 = self.relu3_4(relu3_3)\n",
    "\n",
    "        relu4_1 = self.relu4_1(relu3_4)\n",
    "        relu4_2 = self.relu4_2(relu4_1)\n",
    "        relu4_3 = self.relu4_3(relu4_2)\n",
    "        relu4_4 = self.relu4_4(relu4_3)\n",
    "\n",
    "        relu5_1 = self.relu5_1(relu4_4)\n",
    "        relu5_2 = self.relu5_2(relu5_1)\n",
    "        relu5_3 = self.relu5_3(relu5_2)\n",
    "        relu5_4 = self.relu5_4(relu5_3)\n",
    "        \n",
    "        linear_3 = None\n",
    "        if self.stage == 1:\n",
    "            maxpool = self.maxpool(relu5_4)\n",
    "            x = maxpool.view(maxpool.shape[0],-1)\n",
    "            x = self.class_other(x)\n",
    "            linear_3 = self.linear_3(x)\n",
    "        \n",
    "        out = {\n",
    "            'relu1_1': relu1_1,\n",
    "            'relu1_2': relu1_2,\n",
    "\n",
    "            'relu2_1': relu2_1,\n",
    "            'relu2_2': relu2_2,\n",
    "\n",
    "            'relu3_1': relu3_1,\n",
    "            'relu3_2': relu3_2,\n",
    "            'relu3_3': relu3_3,\n",
    "            'relu3_4': relu3_4,\n",
    "\n",
    "            'relu4_1': relu4_1,\n",
    "            'relu4_2': relu4_2,\n",
    "            'relu4_3': relu4_3,\n",
    "            'relu4_4': relu4_4,\n",
    "\n",
    "            'relu5_1': relu5_1,\n",
    "            'relu5_2': relu5_2,\n",
    "            'relu5_3': relu5_3,\n",
    "            'relu5_4': relu5_4,\n",
    "\n",
    "            'linear_3': linear_3\n",
    "        }\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class BaseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseNetwork, self).__init__()\n",
    "\n",
    "    def init_weights(self, init_type='xavier', gain=0.02):\n",
    "        def init_func(m):\n",
    "            classname = m.__class__.__name__\n",
    "            if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "                if init_type == 'normal':\n",
    "                    nn.init.normal_(m.weight.data, 0.0, gain)\n",
    "                elif init_type == 'xavier':\n",
    "                    nn.init.xavier_normal_(m.weight.data, gain=gain)\n",
    "                elif init_type == 'kaiming':\n",
    "                    nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "                elif init_type == 'orthogonal':\n",
    "                    nn.init.orthogonal_(m.weight.data, gain=gain)\n",
    "\n",
    "                if hasattr(m, 'bias') and m.bias is not None:\n",
    "                    nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "            elif classname.find('BatchNorm2d') != -1:\n",
    "                nn.init.normal_(m.weight.data, 1.0, gain)\n",
    "                nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "        self.apply(init_func)\n",
    "        \n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim, dilation=1, use_spectral_norm=False, use_dropout=False):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        conv_block = [\n",
    "            nn.ReflectionPad2d(dilation),\n",
    "            spectral_norm(nn.Conv2d(in_channels=dim, out_channels=dim, kernel_size=3, padding=0, dilation=dilation, bias=not use_spectral_norm), use_spectral_norm),\n",
    "            nn.InstanceNorm2d(dim, track_running_stats=False),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ReflectionPad2d(1),\n",
    "            spectral_norm(nn.Conv2d(in_channels=dim, out_channels=dim, kernel_size=3, padding=0, dilation=1, bias=not use_spectral_norm), use_spectral_norm),\n",
    "            nn.InstanceNorm2d(dim, track_running_stats=False),\n",
    "         ]\n",
    "        \n",
    "        if use_dropout:\n",
    "            conv_block += [nn.Dropout(0.5)]\n",
    "            \n",
    "        self.conv_block = nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.conv_block(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "def spectral_norm(module, mode=True):\n",
    "    if mode:\n",
    "        return nn.utils.spectral_norm(module)\n",
    "\n",
    "    return module\n",
    "    \n",
    "class ImagineNet(BaseNetwork):\n",
    "    def __init__(self, residual_blocks=8, init_weights=True, in_channels=3, out_channels=3, expand=True):\n",
    "        super(ImagineNet, self).__init__()\n",
    "        \n",
    "        # self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=64, kernel_size=7, stride=1, padding=0),\n",
    "            nn.InstanceNorm2d(64, track_running_stats=False),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128, track_running_stats=False),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256, track_running_stats=False),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        blocks = []\n",
    "        for _ in range(residual_blocks):\n",
    "            block = ResnetBlock(256, 2)\n",
    "            blocks.append(block)\n",
    "\n",
    "        self.middle = nn.Sequential(*blocks)\n",
    "        \n",
    "        self.outer = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(128, track_running_stats=False),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(64, track_running_stats=False),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(in_channels=64, out_channels=3, kernel_size=7, padding=0),\n",
    "        )\n",
    "        self.tanh = nn.Tanh()\n",
    "            \n",
    "        if init_weights:\n",
    "            self.init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.middle(x)\n",
    "        x = self.outer(x)\n",
    "        x = (self.tanh(x) + 1) / 2\n",
    "        return x\n",
    "\n",
    "\n",
    "class Dis_Imagine(BaseNetwork):\n",
    "    def __init__(self, in_channels, use_sigmoid=True, use_spectral_norm=True, init_weights=True):\n",
    "        super(Dis_Imagine, self).__init__()\n",
    "        self.use_sigmoid = use_sigmoid\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(in_channels=in_channels, out_channels=64, kernel_size=4, stride=2, padding=1, bias=not use_spectral_norm), use_spectral_norm),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1, bias=not use_spectral_norm), use_spectral_norm),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1, bias=not use_spectral_norm), use_spectral_norm),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=1, padding=1, bias=not use_spectral_norm), use_spectral_norm),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.conv5 = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(in_channels=512, out_channels=1, kernel_size=4, stride=1, padding=1, bias=not use_spectral_norm), use_spectral_norm),\n",
    "        )\n",
    "\n",
    "        if init_weights:\n",
    "            self.init_weights()\n",
    "     \n",
    "    def forward(self, x):\n",
    "        conv1 = self.conv1(x)\n",
    "        conv2 = self.conv2(conv1)\n",
    "        conv3 = self.conv3(conv2)\n",
    "        conv4 = self.conv4(conv3)\n",
    "        conv5 = self.conv5(conv4)\n",
    "\n",
    "        outputs = conv5\n",
    "        if self.use_sigmoid:\n",
    "            outputs = torch.sigmoid(conv5)\n",
    "\n",
    "        return outputs, [conv1, conv2, conv3, conv4, conv5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dis_Imagine(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (conv4): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (conv5): Sequential(\n",
       "    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "# ImagineNet()\n",
    "Dis_Imagine(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n   ReflectionPad2d-1            [-1, 3, 72, 72]               0\n            Conv2d-2           [-1, 64, 66, 66]           9,472\n    InstanceNorm2d-3           [-1, 64, 66, 66]               0\n              ReLU-4           [-1, 64, 66, 66]               0\n            Conv2d-5          [-1, 128, 33, 33]          73,856\n    InstanceNorm2d-6          [-1, 128, 33, 33]               0\n              ReLU-7          [-1, 128, 33, 33]               0\n            Conv2d-8          [-1, 256, 17, 17]         295,168\n    InstanceNorm2d-9          [-1, 256, 17, 17]               0\n             ReLU-10          [-1, 256, 17, 17]               0\n  ReflectionPad2d-11          [-1, 256, 21, 21]               0\n           Conv2d-12          [-1, 256, 17, 17]         590,080\n   InstanceNorm2d-13          [-1, 256, 17, 17]               0\n             ReLU-14          [-1, 256, 17, 17]               0\n  ReflectionPad2d-15          [-1, 256, 19, 19]               0\n           Conv2d-16          [-1, 256, 17, 17]         590,080\n   InstanceNorm2d-17          [-1, 256, 17, 17]               0\n      ResnetBlock-18          [-1, 256, 17, 17]               0\n  ReflectionPad2d-19          [-1, 256, 21, 21]               0\n           Conv2d-20          [-1, 256, 17, 17]         590,080\n   InstanceNorm2d-21          [-1, 256, 17, 17]               0\n             ReLU-22          [-1, 256, 17, 17]               0\n  ReflectionPad2d-23          [-1, 256, 19, 19]               0\n           Conv2d-24          [-1, 256, 17, 17]         590,080\n   InstanceNorm2d-25          [-1, 256, 17, 17]               0\n      ResnetBlock-26          [-1, 256, 17, 17]               0\n  ReflectionPad2d-27          [-1, 256, 21, 21]               0\n           Conv2d-28          [-1, 256, 17, 17]         590,080\n   InstanceNorm2d-29          [-1, 256, 17, 17]               0\n             ReLU-30          [-1, 256, 17, 17]               0\n  ReflectionPad2d-31          [-1, 256, 19, 19]               0\n           Conv2d-32          [-1, 256, 17, 17]         590,080\n   InstanceNorm2d-33          [-1, 256, 17, 17]               0\n      ResnetBlock-34          [-1, 256, 17, 17]               0\n  ReflectionPad2d-35          [-1, 256, 21, 21]               0\n           Conv2d-36          [-1, 256, 17, 17]         590,080\n   InstanceNorm2d-37          [-1, 256, 17, 17]               0\n             ReLU-38          [-1, 256, 17, 17]               0\n  ReflectionPad2d-39          [-1, 256, 19, 19]               0\n           Conv2d-40          [-1, 256, 17, 17]         590,080\n   InstanceNorm2d-41          [-1, 256, 17, 17]               0\n      ResnetBlock-42          [-1, 256, 17, 17]               0\n  ReflectionPad2d-43          [-1, 256, 21, 21]               0\n           Conv2d-44          [-1, 256, 17, 17]         590,080\n   InstanceNorm2d-45          [-1, 256, 17, 17]               0\n             ReLU-46          [-1, 256, 17, 17]               0\n  ReflectionPad2d-47          [-1, 256, 19, 19]               0\n           Conv2d-48          [-1, 256, 17, 17]         590,080\n   InstanceNorm2d-49          [-1, 256, 17, 17]               0\n      ResnetBlock-50          [-1, 256, 17, 17]               0\n  ReflectionPad2d-51          [-1, 256, 21, 21]               0\n           Conv2d-52          [-1, 256, 17, 17]         590,080\n   InstanceNorm2d-53          [-1, 256, 17, 17]               0\n             ReLU-54          [-1, 256, 17, 17]               0\n  ReflectionPad2d-55          [-1, 256, 19, 19]               0\n           Conv2d-56          [-1, 256, 17, 17]         590,080\n   InstanceNorm2d-57          [-1, 256, 17, 17]               0\n      ResnetBlock-58          [-1, 256, 17, 17]               0\n  ReflectionPad2d-59          [-1, 256, 21, 21]               0\n           Conv2d-60          [-1, 256, 17, 17]         590,080\n   InstanceNorm2d-61          [-1, 256, 17, 17]               0\n             ReLU-62          [-1, 256, 17, 17]               0\n  ReflectionPad2d-63          [-1, 256, 19, 19]               0\n           Conv2d-64          [-1, 256, 17, 17]         590,080\n   InstanceNorm2d-65          [-1, 256, 17, 17]               0\n      ResnetBlock-66          [-1, 256, 17, 17]               0\n  ReflectionPad2d-67          [-1, 256, 21, 21]               0\n           Conv2d-68          [-1, 256, 17, 17]         590,080\n   InstanceNorm2d-69          [-1, 256, 17, 17]               0\n             ReLU-70          [-1, 256, 17, 17]               0\n  ReflectionPad2d-71          [-1, 256, 19, 19]               0\n           Conv2d-72          [-1, 256, 17, 17]         590,080\n   InstanceNorm2d-73          [-1, 256, 17, 17]               0\n      ResnetBlock-74          [-1, 256, 17, 17]               0\n  ConvTranspose2d-75          [-1, 128, 34, 34]         295,040\n   InstanceNorm2d-76          [-1, 128, 34, 34]               0\n             ReLU-77          [-1, 128, 34, 34]               0\n  ConvTranspose2d-78           [-1, 64, 68, 68]          73,792\n   InstanceNorm2d-79           [-1, 64, 68, 68]               0\n             ReLU-80           [-1, 64, 68, 68]               0\n  ReflectionPad2d-81           [-1, 64, 74, 74]               0\n           Conv2d-82            [-1, 3, 68, 68]           9,411\n             Tanh-83            [-1, 3, 68, 68]               0\n================================================================\nTotal params: 10,198,019\nTrainable params: 10,198,019\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.05\nForward/backward pass size (MB): 64.05\nParams size (MB): 38.90\nEstimated Total Size (MB): 103.01\n----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "net = ImagineNet()\n",
    "# net = Dis_Imagine(6)\n",
    "summary(net, (3, 66, 66))\n",
    "# summary(net, (6, 66, 66))"
   ]
  }
 ]
}